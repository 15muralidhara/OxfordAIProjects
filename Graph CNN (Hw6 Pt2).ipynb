{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/15muralidhara/oxcourse/blob/main/Graph%20CNN%20(Hw6%20Pt2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and train a graph convolutional neural network using PyTorch Geometric for the node property prediction task.\n",
        "\n",
        "We will use ogbn-products dataset."
      ],
      "metadata": {
        "id": "G2mlOkawZgDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OGBN-Products"
      ],
      "metadata": {
        "id": "g_Vh9kosaChl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ogbn-products dataset is an undirected and unweighted graph, representing an Amazon product co-purchasing network. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. Node features are generated by extracting bag-of-words features from the product descriptions followed by a Principal Component Analysis to reduce the dimension to 100.\n",
        "\n",
        "The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels."
      ],
      "metadata": {
        "id": "8HFHJC-LaLyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vkP8pA1qBE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec12984-b4f8-4e95-895c-14c1483a52aa"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6d22O6DqGSZ"
      },
      "source": [
        "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8hfxJ-qRg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90cca68d-9323-497b-89bb-88f9751bde4a"
      },
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt20cu118\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910460 sha256=a24ef5bbd1900859157b24fbbe2ca337f0197073247e1d424d28d652d27f3087\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=e4bb03bf156914e6b2b8facda7916c85f01f20a9c02cdb7e4300eb664170249a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import DataLoader\n",
        "import numpy as np\n",
        "from torch_geometric.typing import SparseTensor"
      ],
      "metadata": {
        "id": "3SkS1Mzcbe8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ibJ0ieoIwQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cdd679-1cd6-4a78-c6f3-5e70678ab864"
      },
      "source": [
        "dataset_name = 'ogbn-products'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "# data = data.to(device)\n",
        "# split_idx = dataset.get_idx_split()\n",
        "# train_idx = split_idx['train'].to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|██████████| 1414/1414 [00:17<00:00, 80.17it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 97.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ6I9prjdg57",
        "outputId": "9db3d2cb-7b1c-4ceb-d80b-0dcdf8414ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=2449029, x=[2449029, 100], y=[2449029, 1], adj_t=[2449029, 2449029, nnz=123718280])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is very big and if you try to run it as it is on colab, you may get an out of memory error.\n",
        "\n",
        "One solution is to use batching and train on subgraphs. Here, we will just make a smaller dataset so that we can train it in one go."
      ],
      "metadata": {
        "id": "L7QzFWvS2DvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to have edge indxes to make a subgraph. We can get those from the adjacency matrix.\n",
        "data.edge_index = torch.stack([data.adj_t.__dict__[\"storage\"]._row, data.adj_t.__dict__[\"storage\"]._col])\n",
        "\n",
        "# We will only use the first 100000 nodes.\n",
        "sub_nodes = 100000\n",
        "sub_graph = data.subgraph(torch.arange(sub_nodes))\n",
        "\n",
        "# Update the adjaceny matrix according to the new graph\n",
        "sub_graph.adj_t = SparseTensor(\n",
        "    row=sub_graph.edge_index[0],\n",
        "    col=sub_graph.edge_index[1],\n",
        "    sparse_sizes=None,\n",
        "    is_sorted=True,\n",
        "    trust_data=True,\n",
        ")\n",
        "\n",
        "sub_graph = sub_graph.to(device)\n",
        "\n",
        "sub_graph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Nrhnno6ylE",
        "outputId": "0e20d0cf-3026-4653-d04c-96c5de6a3c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=100000, x=[100000, 100], y=[100000, 1], adj_t=[100000, 100000, nnz=2818046], edge_index=[2, 2818046])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spilt data into train validation and test set\n",
        "split_sizes = [int(sub_nodes*0.8),int(sub_nodes*0.05),int(sub_nodes*0.15)]\n",
        "indices = torch.arange(sub_nodes)\n",
        "np.random.shuffle(indices.numpy())\n",
        "split_idx = {s:t for t,s in zip(torch.split(indices, split_sizes, dim=0), [\"train\", \"valid\", \"test\"])}\n",
        "split_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJr8y1mG7xN0",
        "outputId": "5906b4d0-068e-4467-913d-20cfa941e82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': tensor([36219, 25071, 89645,  ..., 30159, 77955, 70048]),\n",
              " 'valid': tensor([88275, 65023, 89446,  ..., 64010, 89158, 23509]),\n",
              " 'test': tensor([51001, 24995, 66942,  ..., 18649, 19824, 75270])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Feature Length of each node: {data.x.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfHKG2kTcEOe",
        "outputId": "7d6e6f0c-222b-45b4-f054-8e18c326b0cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Length of each node: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "\n",
        "        # input layer\n",
        "        self.conv_layers.append(GCNConv(in_channels, hidden_channels))\n",
        "\n",
        "        # hidden layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.conv_layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        # output layer\n",
        "        self.conv_layers.append(GCNConv(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.conv_layers:\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "in_channels = data.x.shape[1]  # feature dimension of each node\n",
        "hidden_channels = 128\n",
        "out_channels = dataset.num_classes  # no. of output classes\n",
        "num_layers = 3\n",
        "\n",
        "model = GCNModel(in_channels, hidden_channels, out_channels, num_layers).to(device)"
      ],
      "metadata": {
        "id": "m1RLX9bQ1Q9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train_idx = split_idx['train']\n",
        "valid_idx = split_idx['valid']\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(sub_graph.x, sub_graph.edge_index)\n",
        "\n",
        "    target = sub_graph.y[train_idx].squeeze().long()\n",
        "\n",
        "    loss = criterion(out[train_idx], target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def evaluate(split):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    out = model(sub_graph.x, sub_graph.adj_t)\n",
        "    pred = out.argmax(dim=-1, keepdim=True)\n",
        "  correct = pred[split_idx[split]] == sub_graph.y[split_idx[split]]\n",
        "  accuracy = int(correct.sum()) / int(split_idx[split].size(0))\n",
        "  return accuracy\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)\n",
        "    accuracy = evaluate('valid')  # Calculate validation accuracy for the 'valid' split\n",
        "    print(f'Epoch: {epoch:02d}, Validation Accuracy: {accuracy:.4f}%')\n"
      ],
      "metadata": {
        "id": "XzNYB3NSxxWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041acd70-fa3c-4af6-d547-f4f4a30bc948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Validation Accuracy: 0.7562%\n",
            "Epoch: 02, Validation Accuracy: 0.7860%\n",
            "Epoch: 03, Validation Accuracy: 0.7868%\n",
            "Epoch: 04, Validation Accuracy: 0.7854%\n",
            "Epoch: 05, Validation Accuracy: 0.7864%\n",
            "Epoch: 06, Validation Accuracy: 0.7890%\n",
            "Epoch: 07, Validation Accuracy: 0.7922%\n",
            "Epoch: 08, Validation Accuracy: 0.7906%\n",
            "Epoch: 09, Validation Accuracy: 0.7884%\n",
            "Epoch: 10, Validation Accuracy: 0.7870%\n",
            "Epoch: 11, Validation Accuracy: 0.7858%\n",
            "Epoch: 12, Validation Accuracy: 0.7874%\n",
            "Epoch: 13, Validation Accuracy: 0.7892%\n",
            "Epoch: 14, Validation Accuracy: 0.7910%\n",
            "Epoch: 15, Validation Accuracy: 0.7930%\n",
            "Epoch: 16, Validation Accuracy: 0.7924%\n",
            "Epoch: 17, Validation Accuracy: 0.7928%\n",
            "Epoch: 18, Validation Accuracy: 0.7930%\n",
            "Epoch: 19, Validation Accuracy: 0.7918%\n",
            "Epoch: 20, Validation Accuracy: 0.7924%\n",
            "Epoch: 21, Validation Accuracy: 0.7920%\n",
            "Epoch: 22, Validation Accuracy: 0.7912%\n",
            "Epoch: 23, Validation Accuracy: 0.7914%\n",
            "Epoch: 24, Validation Accuracy: 0.7908%\n",
            "Epoch: 25, Validation Accuracy: 0.7908%\n",
            "Epoch: 26, Validation Accuracy: 0.7922%\n",
            "Epoch: 27, Validation Accuracy: 0.7934%\n",
            "Epoch: 28, Validation Accuracy: 0.7938%\n",
            "Epoch: 29, Validation Accuracy: 0.7950%\n",
            "Epoch: 30, Validation Accuracy: 0.7944%\n",
            "Epoch: 31, Validation Accuracy: 0.7930%\n",
            "Epoch: 32, Validation Accuracy: 0.7926%\n",
            "Epoch: 33, Validation Accuracy: 0.7916%\n",
            "Epoch: 34, Validation Accuracy: 0.7912%\n",
            "Epoch: 35, Validation Accuracy: 0.7916%\n",
            "Epoch: 36, Validation Accuracy: 0.7920%\n",
            "Epoch: 37, Validation Accuracy: 0.7920%\n",
            "Epoch: 38, Validation Accuracy: 0.7928%\n",
            "Epoch: 39, Validation Accuracy: 0.7930%\n",
            "Epoch: 40, Validation Accuracy: 0.7936%\n",
            "Epoch: 41, Validation Accuracy: 0.7932%\n",
            "Epoch: 42, Validation Accuracy: 0.7930%\n",
            "Epoch: 43, Validation Accuracy: 0.7928%\n",
            "Epoch: 44, Validation Accuracy: 0.7930%\n",
            "Epoch: 45, Validation Accuracy: 0.7934%\n",
            "Epoch: 46, Validation Accuracy: 0.7930%\n",
            "Epoch: 47, Validation Accuracy: 0.7934%\n",
            "Epoch: 48, Validation Accuracy: 0.7932%\n",
            "Epoch: 49, Validation Accuracy: 0.7934%\n",
            "Epoch: 50, Validation Accuracy: 0.7940%\n"
          ]
        }
      ]
    }
  ]
}